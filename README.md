# Agave Vision API

Production-ready YOLOv8 object detection system for industrial cameras with real-time RTSP streaming and ROI-based alerting.

## Overview

Agave Vision is a complete end-to-end system for training and deploying custom object detection models for industrial video surveillance. The system is organized into **three functional modules** plus production deployment:

1. **[Ingestion](ingestion/)** - Data preparation pipeline (videos â†’ frames â†’ tiles â†’ datasets)
2. **[Training](training/)** - Model training workbench with versioning and evaluation
3. **[Models](models/)** - Model registry for version management and deployment
4. **[Production](production/)** - Containerized microservices for live RTSP streaming and alerting

## Features

### Data Pipeline
- **Video Processing**: Extract frames with deduplication and sharpness filtering
- **Tile Generation**: Sliding window tiling for large images
- **Dataset Builder**: YOLO-ready datasets with train/val/test splits

### Training Workbench
- **Model Training**: YOLOv8 training with hyperparameter management
- **Evaluation**: Metrics computation (mAP, precision, recall)
- **Model Comparison**: Side-by-side comparison of multiple versions

### Model Registry
- **Version Management**: Semantic versioning with metadata tracking
- **Metrics Tracking**: Store evaluation metrics with each version
- **Tagging System**: Organize models by stage (baseline, production, etc.)

### Production Services
- **Real-time Detection**: YOLOv8-based detection for custom classes (object, pine, worker)
- **Multi-Camera Support**: Concurrent RTSP stream processing from multiple cameras
- **ROI-Based Alerting**: Flexible region-of-interest filtering with customizable alert rules
- **Microservice Architecture**: Three containerized services (Inference API, Stream Manager, Alert Router)
- **Alert Delivery**: Pluggable protocols (stdout, webhook, Hikvision integration)
- **Production-Ready**: Docker Compose deployment with GPU support, health checks, and auto-restart

## System Architecture

### Complete Workflow

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       DATA PREPARATION                              â”‚
â”‚                                                                     â”‚
â”‚  Videos â†’ Frames â†’ Tiles â†’ Labels â†’ YOLO Dataset                   â”‚
â”‚  (ingestion module)                                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       TRAINING WORKBENCH                            â”‚
â”‚                                                                     â”‚
â”‚  Train â†’ Evaluate â†’ Compare â†’ Experiment                           â”‚
â”‚  (training module)                                                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       MODEL REGISTRY                                â”‚
â”‚                                                                     â”‚
â”‚  Register â†’ Tag â†’ Version â†’ Deploy                                 â”‚
â”‚  (models module)                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       PRODUCTION SERVICES                           â”‚
â”‚                                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚  â”‚ Inference    â”‚    â”‚ Stream       â”‚    â”‚ Alert        â”‚         â”‚
â”‚  â”‚ API          â”‚    â”‚ Manager      â”‚    â”‚ Router       â”‚         â”‚
â”‚  â”‚ (FastAPI)    â”‚    â”‚ (RTSP        â”‚    â”‚ (Redis       â”‚         â”‚
â”‚  â”‚              â”‚    â”‚  Ingestion)  â”‚    â”‚  Consumer)   â”‚         â”‚
â”‚  â”‚ Port: 8000   â”‚    â”‚              â”‚    â”‚              â”‚         â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚         â”‚                   â”‚                    â”‚                 â”‚
â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                             â”‚                                      â”‚
â”‚                      â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”                              â”‚
â”‚                      â”‚ Redis        â”‚                              â”‚
â”‚                      â”‚ (Message Bus)â”‚                              â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚
â”‚  (production module)                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Quick Start

### Prerequisites

- Python 3.11+
- Docker & Docker Compose (for production deployment)
- NVIDIA GPU with drivers (for GPU training and inference)
- NVIDIA Container Toolkit (for Docker GPU support)

### Installation

```bash
git clone <repository-url>
cd agave-vision-api
python3 -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
pip install -e .[all]
```

### Complete Workflow Example

#### 1. Prepare Training Data (Ingestion Module)

```bash
# Extract frames from videos
./agave-ingest extract-frames \
    --video-dir data/videos \
    --output-dir data/frames \
    --sample-rate 30

# Generate tiles for labeling
./agave-ingest generate-tiles \
    --frames-dir data/frames \
    --output-dir data/tiles_pool \
    --tile-size 640 \
    --overlap 128

# Manually label tiles using your preferred tool (Roboflow, LabelImg, etc.)
# Then build YOLO dataset

./agave-ingest build-dataset \
    --rounds-dir data/tiles_pool/tiles_man \
    --output-dir data/tiles_yolo \
    --classes object pine worker \
    --train-ratio 0.7 \
    --val-ratio 0.15 \
    --test-ratio 0.15
```

See [src/agave_vision/ingestion/README.md](src/agave_vision/ingestion/README.md) for details.

#### 2. Train Model (Training Workbench)

```bash
# Train a new model
./agave-train train \
    --data-yaml configs/yolo_data.yaml \
    --model yolov8n \
    --epochs 100 \
    --batch 16 \
    --device cuda \
    --version v1_baseline

# Evaluate on test set
./agave-train evaluate \
    --model training/runs/v1_baseline/weights/best.pt \
    --split test
```

See [src/agave_vision/training/README.md](src/agave_vision/training/README.md) for details.

#### 3. Register Model (Model Registry)

```python
from agave_vision.models.registry import ModelRegistry

registry = ModelRegistry("models")
registry.register_model(
    version_name="v1_baseline",
    weights_path="training/runs/v1_baseline/weights/best.pt",
    metrics={"map50": 0.842, "map50_95": 0.678},
    config={"model": "yolov8n", "epochs": 100},
    tags=["production"]
)
```

See [src/agave_vision/models/README.md](src/agave_vision/models/README.md) for details.

#### 4. Deploy to Production

**Configure cameras and ROIs**:
Edit `configs/cameras.yaml` and `configs/rois.yaml` with your camera URLs and ROI polygons.

**Update service configuration** to use registered model:
```yaml
# configs/services.yaml
inference:
  model_path: "models/v1_baseline/weights/best.pt"
```

**Deploy with Docker Compose**:

```bash
# Configure environment
cp .env.example .env
# Edit .env with your camera RTSP URLs and settings

# Build and start services
cd production
docker compose build
docker compose up -d

# Check service health
curl http://localhost:8000/health

# Monitor alerts
docker compose logs -f alert-router
```

See production deployment section below for details.

## Configuration

### Camera Configuration (`configs/cameras.yaml`)

```yaml
cameras:
  - id: cam_nave3_hornos
    name: "Nave 3 Hornos B CAM 3"
    rtsp_url: "rtsp://admin:password@192.168.1.10:554/Streaming/Channels/101"
    enabled: true
    fps_target: 5.0
```

### ROI Configuration (`configs/rois.yaml`)

```yaml
cameras:
  - camera_id: cam_nave3_hornos
    forbidden_rois:
      - name: loading_zone
        points:
          - [100, 100]
          - [500, 100]
          - [500, 400]
          - [100, 400]
    allowed_classes: [pine, worker]
    alert_classes: [object]
```

### Alert Configuration (`configs/alerting.yaml`)

```yaml
debounce_window_seconds: 5.0
max_alerts_per_window: 1
protocol: "stdout"  # Options: stdout, webhook, hikvision
webhook_url: null   # Set if using webhook protocol
```

## API Documentation

### Inference API Endpoints

**Health Check**
```bash
GET /health
```

**Run Inference**
```bash
POST /infer
Content-Type: multipart/form-data

Parameters:
- file: Image file (JPEG/PNG)
- conf: Confidence threshold (default: 0.25)

Response:
{
  "detections": [...],
  "inference_time_ms": 45.2,
  "image_size": [640, 480]
}
```

**Get ROI Configurations**
```bash
GET /config/rois
```

**Get Camera Configurations**
```bash
GET /config/cameras
```

## Modules

All modules are organized under `src/agave_vision/` as a unified Python package.

### 1. Ingestion Module

**Location:** `src/agave_vision/ingestion/`

Handles data preparation from raw videos to YOLO-ready datasets.

**Features:**
- Frame extraction with deduplication and sharpness filtering
- Tile generation using sliding window
- Dataset builder with train/val/test splits
- Support for both static data and live streaming (RTSP)

**CLI:**
```bash
./agave-ingest extract-frames --help
./agave-ingest generate-tiles --help
./agave-ingest build-dataset --help

# Or use Python module directly:
python -m agave_vision.ingestion.cli --help
```

**Programmatic Usage:**
```python
from agave_vision.ingestion.static.video_processor import VideoProcessor
from agave_vision.ingestion.static.tile_generator import TileGenerator
from agave_vision.ingestion.static.dataset_builder import DatasetBuilder
```

See [src/agave_vision/ingestion/README.md](src/agave_vision/ingestion/README.md) for complete documentation.

### 2. Training Workbench

**Location:** `src/agave_vision/training/`

Dedicated workspace for training, evaluating, and comparing YOLO models.

**Features:**
- YOLOv8 training with versioning
- Model evaluation (mAP, precision, recall)
- Multi-model comparison
- Training config tracking

**CLI:**
```bash
./agave-train train --help
./agave-train evaluate --help
./agave-train compare --help

# Or use Python module directly:
python -m agave_vision.training.cli --help
```

**Programmatic Usage:**
```python
from agave_vision.training.trainer import YOLOTrainer
from agave_vision.training.evaluator import ModelEvaluator
```

See [src/agave_vision/training/README.md](src/agave_vision/training/README.md) for complete documentation.

### 3. Model Registry

**Location:** `src/agave_vision/models/`

Centralized model versioning and metadata management.

**Features:**
- Version tracking with semantic names
- Metrics and config storage
- Tag-based organization (baseline, production, etc.)
- Deployment artifact management

**Usage:**
```python
from agave_vision.models.registry import ModelRegistry

registry = ModelRegistry("models")
model = registry.get_latest(tag="production")
```

See [src/agave_vision/models/README.md](src/agave_vision/models/README.md) for complete documentation.

### 4. Production Services

**Location:** `src/agave_vision/services/`

Containerized microservices for real-time RTSP streaming and alerting.

**Services:**
- **Inference API** (FastAPI) - REST API for inference requests
- **Stream Manager** - RTSP stream ingestion and processing
- **Alert Router** - Alert delivery with configurable protocols

**Demo Scripts:**
```bash
# Run inference on video file
python -m agave_vision.services.demos.demo_video_infer

# Test ROI alerting
python -m agave_vision.services.demos.infer_alert

# Real-time RTSP stream
python -m agave_vision.services.demos.realtime_yolo_stream
```

See [src/agave_vision/services/README.md](src/agave_vision/services/README.md) and production deployment section for details.

## Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest --cov=src/agave_vision --cov-report=html

# Run specific test file
pytest tests/unit/test_roi.py

# Run integration tests (requires model)
pytest -m integration
```

## Environment Variables

Key environment variables (see `.env.example`):

```bash
# Redis
REDIS_URL=redis://localhost:6379

# Inference
INFERENCE_DEVICE=cuda
INFERENCE_MODEL_PATH=/app/models/yolov8n_pina/exp/weights/best.pt
INFERENCE_CONFIDENCE=0.25

# Camera RTSP URLs (override cameras.yaml)
CAMERA_cam_nave3_hornos_RTSP_URL=rtsp://...

# Alerting
ALERTING_PROTOCOL=stdout
ALERTING_WEBHOOK_URL=https://...
```

## Project Structure

```
agave-vision-api/
â”œâ”€â”€ pyproject.toml                   # Package configuration
â”œâ”€â”€ README.md                        # This file
â”œâ”€â”€ agave-ingest                     # ğŸš€ Ingestion CLI (executable)
â”œâ”€â”€ agave-train                      # ğŸš€ Training CLI (executable)
â”‚
â”œâ”€â”€ src/agave_vision/                # ğŸ“¦ Main Python package
â”‚   â”‚
â”‚   â”œâ”€â”€ ingestion/                   # MODULE 1: Data Ingestion
â”‚   â”‚   â”œâ”€â”€ cli.py                   # CLI implementation
â”‚   â”‚   â”œâ”€â”€ static/                  # Static pipeline classes
â”‚   â”‚   â”‚   â”œâ”€â”€ video_processor.py
â”‚   â”‚   â”‚   â”œâ”€â”€ tile_generator.py
â”‚   â”‚   â”‚   â””â”€â”€ dataset_builder.py
â”‚   â”‚   â”œâ”€â”€ extract_frames.py        # Standalone scripts
â”‚   â”‚   â”œâ”€â”€ generate_tiles.py
â”‚   â”‚   â”œâ”€â”€ clean_tiles.py
â”‚   â”‚   â”œâ”€â”€ build_yolo_dataset.py
â”‚   â”‚   â”œâ”€â”€ split_rounds_from_clean.py
â”‚   â”‚   â”œâ”€â”€ standardize_round_filenames.py
â”‚   â”‚   â”œâ”€â”€ scan_frames_metadata.py
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ training/                    # MODULE 2: Training Workbench
â”‚   â”‚   â”œâ”€â”€ cli.py                   # CLI implementation
â”‚   â”‚   â”œâ”€â”€ trainer.py               # YOLOTrainer class
â”‚   â”‚   â”œâ”€â”€ evaluator.py             # ModelEvaluator class
â”‚   â”‚   â”œâ”€â”€ train_yolo_pina_detector.py
â”‚   â”‚   â”œâ”€â”€ configs/
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ models/                      # MODULE 3: Model Registry
â”‚   â”‚   â”œâ”€â”€ registry.py              # ModelRegistry class
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ services/                    # MODULE 4: Production Services
â”‚   â”‚   â”œâ”€â”€ inference_api/           # FastAPI service
â”‚   â”‚   â”‚   â”œâ”€â”€ app.py
â”‚   â”‚   â”‚   â”œâ”€â”€ routes.py
â”‚   â”‚   â”‚   â”œâ”€â”€ schemas.py
â”‚   â”‚   â”‚   â””â”€â”€ dependencies.py
â”‚   â”‚   â”œâ”€â”€ stream_manager/          # RTSP stream service
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ camera.py
â”‚   â”‚   â”‚   â””â”€â”€ publisher.py
â”‚   â”‚   â”œâ”€â”€ alert_router/            # Alert routing service
â”‚   â”‚   â”‚   â”œâ”€â”€ main.py
â”‚   â”‚   â”‚   â”œâ”€â”€ consumer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ debounce.py
â”‚   â”‚   â”‚   â””â”€â”€ protocols/
â”‚   â”‚   â”œâ”€â”€ demos/                   # Demo scripts
â”‚   â”‚   â”‚   â”œâ”€â”€ demo_video_infer.py
â”‚   â”‚   â”‚   â”œâ”€â”€ infer_alert.py
â”‚   â”‚   â”‚   â””â”€â”€ realtime_yolo_stream.py
â”‚   â”‚   â””â”€â”€ README.md
â”‚   â”‚
â”‚   â”œâ”€â”€ core/                        # Shared core logic
â”‚   â”‚   â”œâ”€â”€ inference.py             # YOLO wrapper
â”‚   â”‚   â”œâ”€â”€ roi.py                   # ROI filtering
â”‚   â”‚   â”œâ”€â”€ alerts.py                # Alert structures
â”‚   â”‚   â””â”€â”€ frames.py                # Frame utilities
â”‚   â”‚
â”‚   â”œâ”€â”€ config/                      # Configuration management
â”‚   â”‚   â”œâ”€â”€ models.py                # Pydantic config models
â”‚   â”‚   â””â”€â”€ loader.py                # Config loader
â”‚   â”‚
â”‚   â””â”€â”€ utils/                       # Shared utilities
â”‚       â”œâ”€â”€ logging.py
â”‚       â””â”€â”€ video.py
â”‚
â”œâ”€â”€ production/                      # ğŸ³ Docker deployment (no code)
â”‚   â”œâ”€â”€ docker-compose.yml           # Service orchestration
â”‚   â”œâ”€â”€ Dockerfile.inference-api
â”‚   â”œâ”€â”€ Dockerfile.stream-manager
â”‚   â””â”€â”€ Dockerfile.alert-router
â”‚
â”œâ”€â”€ models/                          # ğŸ’¾ Model artifacts (gitignored)
â”‚   â”œâ”€â”€ registry.yaml                # Model registry index
â”‚   â””â”€â”€ v1_baseline/                 # Versioned model weights
â”‚       â”œâ”€â”€ weights/best.pt
â”‚       â””â”€â”€ metadata.json
â”‚
â”œâ”€â”€ training/                        # ğŸ“Š Training outputs (gitignored)
â”‚   â””â”€â”€ runs/                        # Training run directories
â”‚
â”œâ”€â”€ configs/                         # âš™ï¸ YAML configurations
â”‚   â”œâ”€â”€ yolo_data.yaml               # YOLO dataset config
â”‚   â”œâ”€â”€ cameras.yaml                 # Camera registry
â”‚   â”œâ”€â”€ rois.yaml                    # ROI definitions
â”‚   â”œâ”€â”€ services.yaml                # Service runtime config
â”‚   â””â”€â”€ alerting.yaml                # Alert rules
â”‚
â”œâ”€â”€ data/                            # ğŸ“ Training data (gitignored)
â”‚   â”œâ”€â”€ videos/                      # Raw videos
â”‚   â”œâ”€â”€ frames/                      # Extracted frames
â”‚   â”œâ”€â”€ tiles_pool/                  # Generated tiles
â”‚   â””â”€â”€ tiles_yolo/                  # YOLO dataset
â”‚
â”œâ”€â”€ tests/                           # ğŸ§ª Test suite
â”‚   â”œâ”€â”€ unit/
â”‚   â””â”€â”€ integration/
â”‚
â””â”€â”€ docs/                            # ğŸ“š Documentation
```

## Development

### Code Quality

```bash
# Format code
black src/ tests/

# Lint code
ruff src/ tests/

# Type check
mypy src/
```

### Adding New Protocol Adapter

1. Create new adapter in `src/agave_vision/services/alert_router/protocols/`
2. Inherit from `ProtocolAdapter` base class
3. Implement `send_alert()` method
4. Register in `protocols/__init__.py`
5. Add configuration to `config/models.py`

## Production Deployment

### Prerequisites

- Docker & Docker Compose installed
- NVIDIA GPU with drivers (for GPU inference)
- NVIDIA Container Toolkit configured
- Trained model registered in model registry
- Camera RTSP URLs available

### Deployment Steps

**1. Prepare Configuration Files**

```bash
# Copy environment template
cp .env.example .env

# Edit camera configuration
vim configs/cameras.yaml

# Edit ROI configuration
vim configs/rois.yaml

# Update service configuration to use registered model
vim configs/services.yaml
```

**2. Verify Model Exists**

Ensure your production model is registered and available:

```bash
ls -lh models/v1_baseline/weights/best.pt
```

**3. Build Docker Images**

```bash
cd production
docker compose build
```

**4. Start Services**

```bash
# Start all services in detached mode
docker compose up -d

# Check service status
docker compose ps
```

**5. Verify Deployment**

```bash
# Check API health
curl http://localhost:8000/health

# Expected response:
# {"status": "healthy", "model_loaded": true}

# View service logs
docker compose logs -f inference-api
docker compose logs -f stream-manager
docker compose logs -f alert-router

# Check GPU usage
nvidia-smi
```

**6. Monitor Alerts**

```bash
# Watch alert output (stdout protocol)
docker compose logs -f alert-router

# Example alert:
# [2025-01-15 14:30:00] ALERT: Camera cam_nave3_hornos - object detected in loading_zone
```

### Service Management

```bash
# Stop all services
docker compose down

# Restart a specific service
docker compose restart stream-manager

# View resource usage
docker stats

# Update service after config change
docker compose up -d --force-recreate stream-manager

# View logs for specific service
docker compose logs --tail=100 -f inference-api
```

### Scaling Considerations

For production deployment with multiple cameras:

1. **GPU Memory**: Monitor VRAM usage with `nvidia-smi`. Each model instance uses ~500MB
2. **CPU Cores**: Stream manager is CPU-intensive for frame decoding
3. **Network Bandwidth**: RTSP streams consume ~2-4 Mbps per camera
4. **Redis Memory**: Alerts are ephemeral, but monitor with `docker stats redis`

### Health Checks

All services include health checks:

```bash
# Check which services are healthy
docker compose ps

# Services show as "healthy" when:
# - inference-api: Model loaded and API responding
# - stream-manager: Connected to at least one camera
# - alert-router: Connected to Redis
```

### Security Considerations

1. **RTSP Credentials**: Use environment variables, not hardcoded in `cameras.yaml`
2. **Network Isolation**: Use Docker networks to isolate services
3. **API Authentication**: Add authentication middleware for production API
4. **Secrets Management**: Use Docker secrets or external vault

### Updating Models in Production

To deploy a new model version:

```bash
# 1. Register new model in registry
python -c "
from agave_vision.models.registry import ModelRegistry
registry = ModelRegistry('models')
registry.tag_model('v2_optimized', tags=['production'])
"

# 2. Update service configuration
vim configs/services.yaml
# Change model_path to: models/v2_optimized/weights/best.pt

# 3. Restart services
cd production
docker compose restart inference-api stream-manager

# 4. Verify new model loaded
curl http://localhost:8000/health
docker compose logs inference-api | grep "Model loaded"
```

## Troubleshooting

### GPU Not Detected

```bash
# Check NVIDIA drivers
nvidia-smi

# Check Docker GPU support
docker run --rm --gpus all nvidia/cuda:11.8.0-base-ubuntu22.04 nvidia-smi
```

### RTSP Connection Issues

```bash
# Test RTSP URL with ffplay
ffplay rtsp://admin:password@192.168.1.10:554/stream

# Check Docker network connectivity
docker compose exec stream-manager ping <camera-ip>
```

### Service Won't Start

```bash
# Check logs
docker compose logs inference-api
docker compose logs stream-manager
docker compose logs alert-router

# Check model file exists
ls -lh models/yolov8n_pina/exp/weights/best.pt

# Verify configurations
python -c "from agave_vision.config.loader import ConfigLoader; ConfigLoader().load_all()"
```

## Performance Tuning

- **Frame sampling**: Adjust `fps_target` in camera config (5-10 FPS recommended)
- **Inference batch size**: Set in `configs/services.yaml` (use 1 for lowest latency)
- **GPU memory**: Use FP16 inference for 2x speedup (`model.half()`)
- **Alert debouncing**: Adjust `debounce_window_seconds` to reduce spam

## License

MIT

## Support

For issues and questions, see `docs/` directory or raise an issue in the repository.

## Acknowledgments

- Built with [Ultralytics YOLOv8](https://github.com/ultralytics/ultralytics)
- Powered by [FastAPI](https://fastapi.tiangolo.com/)
- Deployed with [Docker](https://www.docker.com/)
